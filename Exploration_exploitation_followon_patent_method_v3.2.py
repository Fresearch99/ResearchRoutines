# -*- coding: utf-8 -*-
"""
Author: Dominik Jurek

DATE: 8/26/2021
METHOD: This method is based on the PatentsView_Citation_Measures_v4.py and
        Almeida et al. 2020 and Manso, Balsmeier, Fleming, 2021 (Heterogeneous Innovation over the Business Cycle)
        to define exploratory and exploitory patents.
        !!! First file running on google cloud
        Subversion 1.1: Parallelize execution by splitting main patent data into sub-frames w.r.t. firm identifiers
    VERSION 2: Include additional method for follow-on innovation based on
                Galasso & Schankerman (2015) and Kuhn & Thompson (2019),
                i.e., define follow-on innovation based on forward citations of following years.
        Subversion 2.1: Implement parallel version for followon method and randomize shuffle of list of identifiers
                        !!! Keep output version unchanged since prior execution is on non-intersecting complete set
        Subversion 2.2: Collects subframe results in separate folders and aggregate results to single main frame
        Subversion 2.3: Follow-on citations after Alice date
    VERSION 3: Replace iteration with groupby-aggregate for follow-on innovations, and remove loading routine in execution functions
        Subversion 3.1: in main execution: make the split into subframes more predictable and skip
                        already finished main DF constructions
        Subvesrion 3.2: Adjust submission for hpc.haastech.org cluster, adjust loading procedure of citations data for large memory cluster
"""


# sbatch Exploration_exploitation_followon_patent_method_v3.2.sh
# seff JOB-ID for efficiency details

r'''
#!/bin/bash
#
# Submit this script using: sbatch script-name
#
#SBATCH -p n2d-standard-16 # partitions  c2-standard-30
#SBATCH -c 6 # number of cores
#SBATCH --mem 61G # memory pool for all cores
#SBATCH -o slurm.%N.%j.out # STDOUT
#SBATCH -e slurm.%N.%j.err # STDERR

cd $WORKING_DIR

srun --pty  ~/.conda/envs/py37/bin/python3 Exploration_exploitation_followon_patent_method_v3.1.py

#============================================
# Execution on hpc.haastech.org
#!/usr/bin/bash
#BSUB -n 1
#BSUB -e batch_output_directory/%J.err
#BSUB -o batch_output_directory/%J.out
# cd run _folder
# execute program
source activate py38
/apps/anaconda3/bin/python Exploration_exploitation_followon_patent_method_v3.2.py

#----------------------------
bsub <Exploration_exploitation_followon_patent_method_v3.2.sh


r'''
# %%
#################################################################
# Load Packages
#################################################################


import pandas as pd
import numpy as np
import re
import os

import swifter
import random

import requests
from io import BytesIO
from lxml import html

import zipfile
import csv

import multiprocessing as mp

import scipy.sparse
import pickle
from pandas.api.types import CategoricalDtype

import datetime
from dateutil.relativedelta import relativedelta

#---------------------------------------
# Define source directory for additional data to be loaded
source_directory = r'Patent_portfolio_structuring_source_directory'
#source_directory = r'C:\Users\domin\Google Drive\Preliminary Research\Alice and Innovation Project\Patent Portfolio and Economic Data\Patent_portfolio_structuring_source_directory'


# Define source path for patent related data from PatentsView_Citation_Measures_v4.py
patent_measure_path = 'PatentsView_Citation_Measures_4'


#----------------------------------------
#Current Build
VERSION = 3

# Define number of prior periods to take as reference for know fields and citations
PRIOR_PERIOD_REFERENCE = 5

# Define number of year to measure follow-on innovation after issuance
POST_PERIOD_ISSUE = 4

# Define number of year to measure follow-on innovation after filing
POST_PERIOD_FILING = 7

#----------------------------------------
# Define output path
OUTPUT_PATH = 'Exploration_exploitation_patent_method_' + str(VERSION)

#====================================
# Create Output Path if not already exist
if not os.path.exists(OUTPUT_PATH):
    os.makedirs(OUTPUT_PATH)

pd.set_option('display.max_rows', 400)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 100)


# %%
#################################################################
# Load from prior construction patent related data
#################################################################
print('Load existing constructed citation data and KPSS-IPC Class data', flush=True)


#*********************************************
# Note, we only need a certain subcategories of the large patent_citation file, thus
# load in chunks and only save the relevant portions

neede_columns = [
    'patent_id', 'citation_id', 'filing_date_patentsview_app_dt',
    'section', 'class', 'sub_class', 'maingroup',
    'section_cited','class_cited', 'sub_class_cited', 'maingroup_cited',
    'filing_date_dt', 'issue_date_dt', 'filing_year', 'issue_year', 'permco',
    'filing_date_dt_cited', 'issue_date_dt_cited', 'filing_year_cited',
    'issue_year_cited', 'permco_cited'
    ]

r'''
chunksize = 10e6
patent_citation_with_kpss = pd.DataFrame()
for chunk in pd.read_csv(patent_measure_path+'/PatentsView_patent_citation_4.csv', encoding='utf-8', chunksize=chunksize, low_memory=False):
    patent_citation_with_kpss = patent_citation_with_kpss.append(
        chunk[neede_columns].drop_duplicates(),
        ignore_index=True
        )
r'''
#^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
# Loading in one with large memory cluster
patent_citation_with_kpss = pd.read_csv(patent_measure_path+'/PatentsView_patent_citation_4.csv',
    encoding='utf-8', low_memory=False,
    usecols=neede_columns)
#patent_citation_with_kpss = pd.read_csv(r"C:\Users\domin\Desktop\PatentsView_patent_citation_4.csv", nrows=1e6)
#patent_citation_with_kpss = pd.read_csv(r"C:\Users\domin\Desktop\test_PatentsView_patent_citation_4.csv")

#*********************************************

kpss_ipc_data = pd.read_csv(patent_measure_path+'/KPSS_ipc_classes_4.csv',
                            encoding='utf-8', low_memory=False)


#kpss_ipc_data = pd.read_csv(r"C:\Users\domin\Desktop\KPSS_ipc_classes_4.csv", nrows=1e6)
#kpss_ipc_data = pd.read_csv(r"C:\Users\domin\Desktop\KPSS_ipc_classes_4.csv")
# %%
#=====================================================
# Post loading data structuring
#=====================================================
# First, structure data
patent_citation_with_kpss['permco'] = pd.to_numeric(
        patent_citation_with_kpss.permco,
        downcast = 'integer', errors = 'coerce')

patent_citation_with_kpss['permco_cited'] = pd.to_numeric(
    patent_citation_with_kpss.permco_cited,
    downcast = 'integer', errors = 'coerce')


patent_citation_with_kpss['filing_date_dt'] = pd.to_datetime(
    patent_citation_with_kpss['filing_date_dt'], errors='coerce')

patent_citation_with_kpss['issue_date_dt'] = pd.to_datetime(
    patent_citation_with_kpss['issue_date_dt'], errors='coerce')

patent_citation_with_kpss['filing_date_patentsview_app_dt'] = pd.to_datetime(
    patent_citation_with_kpss['filing_date_patentsview_app_dt'], errors='coerce')


patent_citation_with_kpss['filing_date_dt_cited'] = pd.to_datetime(
    patent_citation_with_kpss['filing_date_dt_cited'], errors='coerce')

patent_citation_with_kpss['issue_date_dt_cited'] = pd.to_datetime(
    patent_citation_with_kpss['issue_date_dt_cited'], errors='coerce')

#-------------------------------------
# Drop na values from class definition and define date
kpss_ipc_data = kpss_ipc_data[~kpss_ipc_data.maingroup.isna()]

# For annual technology proximity calculation
kpss_ipc_data['filing_date_dt'] = pd.to_datetime(kpss_ipc_data['filing_date_dt'],
                                                 errors='coerce')
kpss_ipc_data['quarter_end_date'] = kpss_ipc_data.filing_date_dt + pd.offsets.QuarterEnd(0)
kpss_ipc_data = kpss_ipc_data.reset_index(drop=True)


# %%
#################################################################
# Implementation of follow-on innovation based on citation
#################################################################
# This follows Galasso-Schankerman (2015), and for the distrinction between
# post-issue and post-filing Kuhn & Thompson (2019).
# The goal is to count the patent follow-on citation (for patents from same and different assignee)

def followon_citation_based(follow_reference_period, type_year, patent_data_frame, patent_data_frame_counter, suboutput_dir):
    r'''PURPOSE:  Method to count for each patent the follow-on patents citing the focal patent
                 in the respective reference period; separately for firms of same filer and other filers
                 Reference period limits to patents filed within that timeframe
        INPUTS:
            follow_reference_period: define number of years after issue/filing to count citations
            type_year: reference to patent issue year of filing year (Kuhn & Thompson 2019)
                        can be 'issue' or 'filing'
            patent_data_frame: the local subframe from the main dataframe 'patent_citation_with_kpss'. This
                main frame is split into equal frames w.r.t. firm identifiers to parallelize the execution
            patent_data_frame_counter: which subframe is executed in this thread, useful to limit
                overriding saved csv by saving sub-results.
            suboutput_dir: directory where to save .csv result outputs
        OUTPUT:
            Files of form:
            'Followon_citation__type_'+str(type_year)+'__following_'+str(follow_reference_period)+ \
                '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv'
    r'''
    print('Begin routine for follow-on innvation measure for {0} following reference periods after {1}, subframe {2}'.\
          format(str(follow_reference_period), str(type_year), str(patent_data_frame_counter)), flush=True)


    #patent_data_frame = patent_citation_with_kpss
    #follow_reference_period = 5
    #type_year = 'issue'
    #patent_data_frame_counter = 'complete'
    #===========================================================
    if not('Followon_citation__type_'+str(type_year)+'__following_'+str(follow_reference_period)+ \
           '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv' in os.listdir(suboutput_dir)):

        # Define also self-citation
        patent_data_frame['self_citation'] = (
            (patent_data_frame.permco == patent_data_frame.permco_cited) & \
            (patent_data_frame.permco.notna()) & (patent_data_frame.permco_cited.notna()) & \
            (patent_data_frame.permco!='') & (patent_data_frame.permco_cited!='')
            ).astype(int)


        # Limit to patent owned by firm
        patent_data_frame = patent_data_frame[~patent_data_frame.permco_cited.isna()]

        print('total number of searchable patent IDs: {0}; subframe {1}'.\
              format(str(len(patent_data_frame.citation_id.dropna().unique())), str(patent_data_frame_counter)), flush=True)
        #------------------------------------------------------------------
        # Aggregate frames that have non-missing main_execution_exploration_exploitation_citation_based
        patent_data_frame = patent_data_frame[~patent_data_frame.citation_id.isna()]

        # Restrict also to valid dates
        patent_data_frame = patent_data_frame[~patent_data_frame[type_year+'_date_dt_cited'].isnull()]

        # Define reference date from which on to measure count of citations (filing or issue data)
        patent_data_frame['last_date_follow_period'] = patent_data_frame.apply(
            lambda row: row[type_year+'_date_dt_cited'] + relativedelta(years=+follow_reference_period), axis=1)

        #------------------------------------------------------------------
        # Limit to following years after issue or filing type
        patent_data_frame_for_aggregation = patent_data_frame[
            (patent_data_frame['filing_date_patentsview_app_dt'] >= patent_data_frame[type_year+'_date_dt_cited']) &\
                (patent_data_frame['filing_date_patentsview_app_dt'] <= patent_data_frame['last_date_follow_period'])
            ]

        #----------------------------------------------------------
        # Define output frame to merge into aggregate results
        follow_on_df = patent_data_frame_for_aggregation[
            ['permco_cited', 'citation_id',
            'filing_date_dt_cited', 'issue_date_dt_cited',
            'filing_year_cited', 'issue_year_cited',
            'last_date_follow_period']].drop_duplicates()

        follow_on_df['follow_period_length'] = follow_reference_period
        follow_on_df['reference_type'] = type_year

        #----------------------------------------------------------
        # Aggregate for cited patents and add to data frame
        total_followon_citation = patent_data_frame_for_aggregation.groupby(['citation_id'])['patent_id'].count().\
            reset_index(drop=False).rename(columns={'patent_id':'total_followon_citation'})
        self_followon_citation = patent_data_frame_for_aggregation[
            patent_data_frame_for_aggregation.self_citation==1].\
            groupby(['citation_id'])['patent_id'].size().\
            reset_index(drop=False).rename(columns={'patent_id':'self_followon_citation'})

        #------------------------------------------------------
        # Merge and output
        follow_on_df = follow_on_df.merge(
            total_followon_citation,
            on=['citation_id'],
            how='left').merge(
                self_followon_citation,
                on=['citation_id'],
                how='left').fillna(0)

        #follow_on_df.citation_id.value_counts().value_counts()
        # => unique

        # Save output
        follow_on_df.to_csv(
            suboutput_dir+'/Followon_citation__type_'+str(type_year)+'__following_'+str(follow_reference_period)+ \
           '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv',
            index=False, encoding='utf-8')

    else:
        print('\tFollowon_citation__type_'+str(type_year)+'__following_'+str(follow_reference_period)+ \
           '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+': already in output, load', flush=True)

    print('End routine for follow-on innvation measure for {0} following reference periods after {1}, subframe {2}'.\
          format(str(follow_reference_period), str(type_year), str(patent_data_frame_counter)), flush=True)
    return


#-----------------------------------------------------------
# Linear Execution
#-----------------------------------------------------------
# => Very slow!
#followon_citation_based(follow_reference_period=4, type_year='issue',
#                        patent_data_frame=patent_citation_with_kpss, patent_data_frame_counter='complete')
#followon_citation_based(follow_reference_period=5, type_year='issue',
#                        patent_data_frame=patent_citation_with_kpss, patent_data_frame_counter='complete')
#followon_citation_based(follow_reference_period=7, type_year='filing',
#                        patent_data_frame=patent_citation_with_kpss, patent_data_frame_counter='complete')

# %%
#################################################################
# Implementation of follow-on citation before and after Alice
#################################################################
# This follows Galasso-Schankerman (2015)
# The goal is to count the patent follow-on citation of patents filed in
# the five years post-Alice and between issue and Alice as pre-period

def followon_Alice(follow_reference_period, patent_data_frame, patent_data_frame_counter, suboutput_dir):
    r'''PURPOSE:  Method to count for each patent the follow-on patents citing the focal patent
                 within the follow_reference_period after Alice (June 19, 2014) and the time
                 between issue and Alice as pre-period
        INPUTS:
            follow_reference_period: define number of years after Alice to count citations
            patent_data_frame: the local subframe from the main dataframe 'patent_citation_with_kpss'. This
                main frame is split into equal frames w.r.t. firm identifiers to parallelize the execution
            patent_data_frame_counter: which subframe is executed in this thread, useful to limit
                overriding saved csv by saving sub-results.
            suboutput_dir: directory where to save .csv result outputs
        OUTPUT:
            Files of form:
            'Followon_citation_Alice__following_'+str(follow_reference_period)+ \
                '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv'
    r'''
    print('Begin routine for follow-on innvation measure for {0} following reference periods after Alice and prior, subframe {1}'.\
          format(str(follow_reference_period), str(patent_data_frame_counter)), flush=True)

    # follow_reference_period = 5
    #patent_data_frame = patent_citation_with_kpss

    # Define Alice date
    alice_date = datetime.datetime(2014, 6, 19)
    #===========================================================
    if not('Followon_citation_Alice__following_'+str(follow_reference_period)+ \
           '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv' in os.listdir(suboutput_dir)):

        # Define also self-citation
        patent_data_frame['self_citation'] = (
            (patent_data_frame.permco == patent_data_frame.permco_cited) & \
            (patent_data_frame.permco.notna()) & (patent_data_frame.permco_cited.notna()) & \
            (patent_data_frame.permco!='') & (patent_data_frame.permco_cited!='')
            ).astype(int)


        # Limit to patent owned by firm
        patent_data_frame = patent_data_frame[~patent_data_frame.permco_cited.isna()]

        print('total number of searchable patent IDs: {0}; subframe {1}'.\
              format(str(len(patent_data_frame.citation_id.dropna().unique())), str(patent_data_frame_counter)), flush=True)

        #------------------------------------------------------------------
        # Aggregate frames that have non-missing main_execution_exploration_exploitation_citation_based
        patent_data_frame = patent_data_frame[~patent_data_frame.citation_id.isna()]

        # Define reference date from which on to measure count of citations (filing or issue data)
        future_reference_date = alice_date + relativedelta(years=+follow_reference_period)

        #------------------------------------------------------------------
        # Limit to following years after issue or filing type

        post_patent_data_frame_for_aggregation = patent_data_frame[
            (patent_data_frame['filing_date_patentsview_app_dt'] >= alice_date) &\
                (patent_data_frame['filing_date_patentsview_app_dt'] <= future_reference_date)
            ]

        pre_patent_data_frame_for_aggregation = patent_data_frame[
            (patent_data_frame['filing_date_patentsview_app_dt'] >= patent_data_frame['issue_date_dt_cited']) &\
                (patent_data_frame['filing_date_patentsview_app_dt'] < alice_date)
            ]

        #----------------------------------------------------------
        # Define output frame to merge into aggregate results
        follow_on_df = patent_data_frame[
            ['permco_cited', 'citation_id',
            'filing_date_dt_cited', 'issue_date_dt_cited',
            'filing_year_cited', 'issue_year_cited']].drop_duplicates()

        follow_on_df['follow_period_length'] = follow_reference_period
        follow_on_df['Alice_decision_date'] = alice_date
        follow_on_df['last_date_follow_period'] = future_reference_date

        #----------------------------------------------------------
        # Aggregate for cited patents and add to data frame
        total_followon_postAlice_citation = post_patent_data_frame_for_aggregation.groupby(['citation_id'])['patent_id'].size().\
            reset_index(drop=False).rename(columns={'patent_id':'total_followon_postAlice_citation'})
        self_followon_postAlice_citation = post_patent_data_frame_for_aggregation[
                post_patent_data_frame_for_aggregation.self_citation==1].\
            groupby(['citation_id'])['patent_id'].size().\
            reset_index(drop=False).rename(columns={'patent_id':'self_followon_postAlice_citation'})

        total_followon_preAlice_citation = pre_patent_data_frame_for_aggregation.groupby(['citation_id'])['patent_id'].size().\
            reset_index(drop=False).rename(columns={'patent_id':'total_followon_preAlice_citation'})
        self_followon_preAlice_citation = pre_patent_data_frame_for_aggregation[
                pre_patent_data_frame_for_aggregation.self_citation==1].\
            groupby(['citation_id'])['patent_id'].size().\
            reset_index(drop=False).rename(columns={'patent_id':'self_followon_preAlice_citation'})

        #------------------------------------------------------
        # Merge and output
        follow_on_df = follow_on_df.merge(
            total_followon_postAlice_citation,
            on=['citation_id'],
            how='left').merge(
                self_followon_postAlice_citation,
                on=['citation_id'],
                how='left').merge(
                    total_followon_preAlice_citation,
                    on=['citation_id'],
                    how='left').merge(
                        self_followon_preAlice_citation,
                        on=['citation_id'],
                        how='left').fillna(0)

        # Save output
        follow_on_df.to_csv(
            suboutput_dir+'/Followon_citation_Alice__following_'+str(follow_reference_period)+ \
           '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv',
            index=False, encoding='utf-8')

    else:
        print('\tFollowon_citation_Alice__following_'+str(follow_reference_period)+ \
           '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+': already in output, load', flush=True)

    print('End routine for follow-on innvation measure for {0} following reference periods after Alice and prior, subframe {1}'.\
          format(str(follow_reference_period), str(patent_data_frame_counter)), flush=True)
    return


# %%
#################################################################
# Implementation of patent class based measures from Manso, Balsmeier, Fleming (2021)
#################################################################
# Split the underlying dataframe into equal parts to parallelize the execution of the main function
def exploration_exploitation_class_based(prior_reference_periods, patent_data_frame, patent_data_frame_counter, suboutput_dir):
    r'''PURPOSE: Method to classify for each firm its filed patents of being exploratory or exploitory.
                based on patent classes filed in pre-period.
        INPUTS:
            prior_reference_periods: define number of prior filing years to measure known classes
            patent_data_frame: the local subframe from the main dataframe 'kpss_ipc_data'. This
                main frame is split into equal frames w.r.t. firm identifiers to parallelize the execution
            patent_data_frame_counter: which subframe is executed in this thread, useful to limit
                overriding saved csv by saving sub-results.
            suboutput_dir: directory where to save .csv result outputs
        OUTPUT:
            Output subfolder "'exploration_exploitation_classBased_outputs_v'+str(VERSION)" in OUTPUT_PATH with files of form:
            'Exploration_exploitation_classBased__prior_'+str(prior_reference_periods)+
            '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv'
    r'''

    print('Begin routine for exploration-exploitation patent class measures for {0} prior reference periods, subframe {1}'.\
          format(str(prior_reference_periods), str(patent_data_frame_counter)), flush=True)

    #patent_data_frame = kpss_ipc_data[kpss_ipc_data.permco.isin(subframe_identifiers)]
    #patent_data_frame_counter = 3
    #prior_reference_periods = 5
    #======================================================
    print('\tFrom Manso, Balsmeier, Fleming, 2021, definition.', flush=True)
    # We define a patent as explorative if its main technology class is new to the firm and exploitative if
    # its main technology class is known to the firm, taking all patenting of a given firm during the
    # 5 years prior to the year a given patent is applied for.

    if not('Exploration_exploitation_classBased__prior_'+str(prior_reference_periods)+
            '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv' in os.listdir(suboutput_dir)):

        # Define empty output dataframe
        mbf_2021_exploration_exploitation = pd.DataFrame()

        print('total number of searchable firm IDs: {0}; subframe {1}'.\
              format(str(len(patent_data_frame.permco.unique())), str(patent_data_frame_counter)), flush=True)
        #------------------------------------------------------------------
        # iterate through each firm, sort and use prior five years of application as reference
        for firm_id in patent_data_frame.permco.dropna().unique():
            #print(firm_id)
            #firm_id = patent_data_frame.permco.unique()[4]

            print('\t current firm class based measure: {0}'.format(str(firm_id)), flush=True)
            current_kpss_patents = patent_data_frame[patent_data_frame.permco==firm_id]

            #------------------------------------------------------------------
            # Define each filing year and collect prior years of filing
            for current_filing_year in current_kpss_patents.filing_year.unique():
                #current_filing_year = current_kpss_patents.filing_year.unique()[10]
                #current_filing_year = current_kpss_patents.filing_year.unique().min()

                #------------------------------------------------------------------
                prior_filing_year = current_kpss_patents[
                    (current_kpss_patents.filing_year < current_filing_year) & \
                    (current_kpss_patents.filing_year >= (current_filing_year-prior_reference_periods))]

                # Define known classes
                known_section = set(prior_filing_year['section'])
                known_class = set(prior_filing_year['class'])
                known_sub_class = set(prior_filing_year['sub_class'])
                known_maingroup = set(prior_filing_year['maingroup'])

                #------------------------------------------------------------------
                # Iterate now through the current patents and see if their class has appeared in prior filing
                append_current_patents = current_kpss_patents[current_kpss_patents.filing_year==current_filing_year].copy()

                append_current_patents['known_section'] = append_current_patents['section'].isin(known_section).astype(int)
                append_current_patents['known_class'] = append_current_patents['class'].isin(known_class).astype(int)
                append_current_patents['known_sub_class'] = append_current_patents['sub_class'].isin(known_sub_class).astype(int)
                append_current_patents['known_maingroup'] = append_current_patents['maingroup'].isin(known_maingroup).astype(int)

                #-----------------------------------------------------------------
                # Add also number of prior periods, patents, and classes
                append_current_patents['num_ref_years'] = len(prior_filing_year.filing_year.unique())
                append_current_patents['num_prior_patents'] = len(prior_filing_year.patent_id.unique())

                append_current_patents['num_prior_section'] = len(known_section)
                append_current_patents['num_prior_class'] = len(known_class)
                append_current_patents['num_prior_sub_class'] = len(known_sub_class)
                append_current_patents['num_prior_maingroup'] = len(known_maingroup)

                #-----------------------------------------------------------------
                # Append to output dataframe
                mbf_2021_exploration_exploitation = mbf_2021_exploration_exploitation.append(
                    append_current_patents, ignore_index=True)


        #**********************************************************
        # Save output
        mbf_2021_exploration_exploitation.to_csv(
            suboutput_dir+'/Exploration_exploitation_classBased__prior_'+str(prior_reference_periods)+
            '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv',
            index=False, encoding='utf-8')

    else:
        print('\tExploration_exploitation_classBased__prior_'+str(prior_reference_periods)+
            '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+': already in output, load', flush=True)

    print('End routine for exploration-exploitation patent class measures for {0} prior reference periods, subframe {1}'.\
          format(str(prior_reference_periods), str(patent_data_frame_counter)), flush=True)

    return

# %%
#################################################################
# Implementation of citation based measures from Almeida et al, 2020
#################################################################
def exploration_exploitation_citation_based(prior_reference_periods, patent_data_frame, patent_data_frame_counter, suboutput_dir):
    r'''PURPOSE: Method to classify for each firm its filed patents of being exploratory or exploitory.
                based on citations in patent filings in pre-period.
        INPUTS:
            prior_reference_periods: define number of prior filing years to measure known citations
            patent_data_frame: the local subframe from the main dataframe 'patent_citation_with_kpss'. This
                main frame is split into equal frames w.r.t. firm identifiers to parallelize the execution
            patent_data_frame_counter: which subframe is executed in this thread, useful to limit
                overriding saved csv by saving sub-results.
            suboutput_dir: directory where to save .csv result outputs
        OUTPUT:
            Files of form:
            'Exploration_exploitation_citationsBased__prior_'+str(prior_reference_periods)+
            '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv'
    r'''

    print('Begin routine for exploration-exploitation citations-based measures for {0} prior reference periods, subframe {1}'.\
          format(str(prior_reference_periods), str(patent_data_frame_counter)), flush=True)

    #======================================================
    print('\tFrom Almeida et al. 2020, definition.', flush=True)
        # Exploratory patent ratio:
        # For each patent, we categorize each backward citation as a “new citation” if
        # it has never been cited by the firm’s other patents filed over the past five years.
        # If a patent’s backward citations contain 90% or higher of new citations,
        # it is classified as an exploratory patent.

        # Exploitative patent ratio:
        # For each patent, we categorize each backward citation as an “old citation” if
        # it has been cited by the firm’s other patents filed over the past five years or
        # if it is the firm’s prior patent. If a patent’s backward citations contain 90% or
        # higher of old citations, it is classified as an exploitative patent.

    if not('Exploration_exploitation_citationsBased__prior_'+str(prior_reference_periods)+
            '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv' in os.listdir(suboutput_dir)):

        #-------------------------------------------------
        # Define empty output dataframe
        citation_exploration_exploitation = pd.DataFrame()

        print('total number of searchable firm IDs: {0}; subframe {1}'.\
              format(str(len(patent_data_frame.permco.unique())), str(patent_data_frame_counter)), flush=True)

        #------------------------------------------------------------------
        # iterate through each firm, sort and use prior years of application as reference
        for firm_id in patent_data_frame.permco.dropna().unique():
            #firm_id = patent_data_frame.permco.dropna().unique()[4]

            print('\t current firm citation based measure: {0}'.format(str(firm_id)), flush=True)
            current_citation_patents = patent_data_frame[patent_data_frame.permco==firm_id]

            #------------------------------------------------------------------
            # Define each filing year and collect prior years of filing
            for current_filing_year in current_citation_patents.filing_year.dropna().unique():
                #current_filing_year = current_citation_patents.filing_year.dropna().unique()[10]
                #current_filing_year = current_citation_patents.filing_year.dropna().unique().min()

                #------------------------------------------------------------------
                prior_filing_year = current_citation_patents[
                    (current_citation_patents.filing_year < current_filing_year) & \
                    (current_citation_patents.filing_year >= (current_filing_year-prior_reference_periods))]

                # Define prior citation IDs !!!I limit here to patent citations!!!
                known_citation = set(prior_filing_year['citation_id'])

                #***********************************************************
                # Include also self-citations
                self_citations = current_citation_patents[
                    (current_citation_patents.permco == current_citation_patents.permco_cited) & \
                        (current_citation_patents.permco.notna()) & (current_citation_patents.permco_cited.notna()) & \
                          (current_citation_patents.permco!='') & (current_citation_patents.permco_cited!='')
                    ][['citation_id', 'patent_id']]

                known_self_citations = set(self_citations['citation_id'])

                known_citation = known_citation.union(known_self_citations)

                #------------------------------------------------------------------
                # Iterate now through the current patents and see if the respective citation is prior known
                this_filing_year_current_patents = current_citation_patents[current_citation_patents.filing_year==current_filing_year].copy()

                this_filing_year_current_patents['known_citation'] = this_filing_year_current_patents['citation_id'].isin(known_citation).astype(int)

                #----------------------------------------------------------------
                # Aggregate now on patent id
                backward_citations = this_filing_year_current_patents[
                    ['citation_id', 'patent_id', 'known_citation']
                    ].drop_duplicates().groupby(['patent_id']).\
                    agg(num_backward_citations =  ('citation_id', len),
                        old_citations = ('known_citation', sum)).reset_index(drop=False)

                backward_citations['new_citations'] = backward_citations['num_backward_citations'] - backward_citations['old_citations']
                #this_filing_year_current_patents[this_filing_year_current_patents.patent_id==7297164]

                #-----------------------------------------------------------------
                # Add also number of prior periods, patents, and overall unique and total citations
                backward_citations['num_ref_years'] = len(prior_filing_year.filing_year.dropna().unique())
                backward_citations['num_prior_patents'] = len(prior_filing_year.patent_id.dropna().unique())
                backward_citations['num_current_patents'] = len(this_filing_year_current_patents.patent_id.dropna().unique())

                backward_citations['num_unique_prior_citations'] = len(prior_filing_year.citation_id.dropna().unique())
                backward_citations['num_total_prior_citations'] = len(prior_filing_year.citation_id.dropna())

                backward_citations['num_unique_current_citations'] = len(this_filing_year_current_patents.citation_id.dropna().unique())
                backward_citations['num_total_current_citations'] = len(this_filing_year_current_patents.citation_id.dropna())

                backward_citations['permco'] = firm_id
                backward_citations['filing_year'] = current_filing_year

                #-----------------------------------------------------------------
                # Append to output dataframe
                citation_exploration_exploitation = citation_exploration_exploitation.append(
                    backward_citations, ignore_index=True)

        #**********************************************************
        # Save output
        citation_exploration_exploitation.to_csv(
            suboutput_dir+'/Exploration_exploitation_citationsBased__prior_'+str(prior_reference_periods)+
            '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'.csv',
            encoding='utf-8', index=False)

    else:
        print('\tExploration_exploitation_citationsBased__prior_'+str(prior_reference_periods)+
            '_years__v'+str(VERSION)+'__subframe_'+str(patent_data_frame_counter)+'  already in output, load', flush=True)

    print('End routine for exploration-exploitation citations-based measures for {0} prior reference periods, subframe {1}'.\
          format(str(prior_reference_periods), str(patent_data_frame_counter)), flush=True)

    return

# %%
#-------------------------------------------------
# Execution
#-------------------------------------------------

def split_list(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

#print(list(split_list([1,2,3,4,5,6,7], 2)))
#*****************************************
#n_cores = mp.cpu_count()
n_cores = 5
print('\t\t Number of Cores: ' + str(n_cores), flush=True)

#len(kpss_ipc_data.permco.unique()) => 8509

#=======================================================
def main_execution_followon_Alice(num_subframe, follow_period_length):
    r'''Splits main data frame 'patent_citation_with_kpss' and sends to execution function
        num_subframe gives number of unique identifiers in each subframe
        follow_period_length is also passed on;

        Save in subframes into output subfolder "'followon_citation_Alice_outputs__following_'
                +str(follow_period_length)+'_years_'+str(VERSION)"
        and concatenated main DF into "'Followon_citation_Alice__following_'
            +str(follow_period_length)+'_years__mainDF_v'+str(VERSION)"
    r'''
    print('Start main routine for follow-on Alice execution', flush=True)

    if not('Followon_citation_Alice__following_'+ str(follow_period_length)+\
           '_years__mainDF_v'+str(VERSION)+'.csv' in os.listdir(OUTPUT_PATH)):

        # Split the firm identifiers into equally sized portions of num_subframe size
        identifiers_list = patent_citation_with_kpss.citation_id.dropna().unique()

        # Sort in ascending order to replicate subframes in predicatable way
        identifiers_list.sort()

        separated_identifiers_list = list(split_list(identifiers_list, num_subframe))

        #-----------------------------------
        # Create suboutput folder if not already exists
        #-----------------------------------
        suboutput_dir = OUTPUT_PATH+'//followon_citation_Alice_outputs__following_' + \
            str(follow_period_length)+'_years_'+str(VERSION)
        if not os.path.exists(suboutput_dir):
            os.makedirs(suboutput_dir)

        #----------------------------------------------
        # Split the main dataframe based on respective sub-sample and forward to pool
        pool = mp.Pool(n_cores)

        for subframe_identifiers_number in range(len(separated_identifiers_list)):
            #print(subframe_identifiers_number)
            #subframe_identifiers_number = 3

            # collect identifiers for iteration
            subframe_identifiers = separated_identifiers_list[subframe_identifiers_number]

            #***********************************************
            # Save subframe identifiers
            with open(suboutput_dir+'/subframe_identifiers_followon_Alice__subframe_' + str(subframe_identifiers_number)+'.pkl', 'wb') as f:
                pickle.dump(subframe_identifiers, f)

            #***********************************************
            # Forward subframe to method
            pool.apply_async(
                        followon_Alice,
                        args=(
                            follow_period_length,
                            patent_citation_with_kpss[patent_citation_with_kpss.citation_id.isin(subframe_identifiers)],
                            subframe_identifiers_number,
                            suboutput_dir)
                    )

        pool.close()
        pool.join()

        #--------------------------------------------
        # Collect sub-frames and output under OUTPUT_PATH
        #--------------------------------------------
        print('\t Collect and save subframes for follow-on Alice execution', flush=True)
        file_list = [suboutput_dir+'//'+f for f in os.listdir(suboutput_dir) if \
                 bool(re.search('.csv', f)) & \
                     bool(re.search('Followon_citation_Alice_', f)) & \
                         bool(re.search(r'subframe_(\d+)', f))]

        # create dictionary with subframes as keys
        file_dir = {}
        for f in file_list:
            file_dir[int(re.search(r'subframe_(\d+)', f).group(1))] = f

        # Load and concatentate
        master_output_df = pd.DataFrame()
        for k in file_dir.keys():

            append_df = pd.read_csv(file_dir[k], low_memory=False)
            append_df['subframe_number'] = k

            master_output_df = master_output_df.append(
                append_df, ignore_index=True)

        # Save mainframe
        master_output_df.to_csv(OUTPUT_PATH+'//Followon_citation_Alice__following_'+\
                                str(follow_period_length)+'_years__mainDF_v'+str(VERSION)+'.csv',
                                index=False, encoding = 'utf-8')
    else:
        print('\tMain_DF from collecting results from follow-on Alice execution already in output directory; skip', flush=True)

    print('End main routine for follow-on  Alice execution', flush=True)
    return

#=======================================================
def main_execution_followon(num_subframe, follow_period_length, type_year):
    r'''Splits main data frame 'patent_citation_with_kpss' and sends to execution function
        num_subframe gives number of unique identifiers in each subframe
        follow_period_length and type_year (issue or filing) are also passed on;

        Save in subframes into output subfolder "'followon_citation_outputs__type_'+ str(type_year)+\
            '__following_'+str(follow_period_length)+'_years__v'+str(VERSION)"
        and concatenated main DF into "'Followon_citation__type_'+str(type_year)+\
            '__following_'+str(follow_period_length)+ \
           '_years__mainDF_v'+str(VERSION)"
    r'''
    print('Start main routine for follow-on execution', flush=True)

    if not('Followon_citation__type_'+str(type_year)+'__following_'+\
           str(follow_period_length)+'_years__mainDF_v'+str(VERSION)+'.csv' in os.listdir(OUTPUT_PATH)):

        # Split the firm identifiers into equally sized portions of num_subframe size
        identifiers_list = patent_citation_with_kpss.citation_id.dropna().unique()

        # Sort in ascending order to replicate subframes in predicatable way
        identifiers_list.sort()

        separated_identifiers_list = list(split_list(identifiers_list, num_subframe))

        #-----------------------------------
        # Create suboutput folder if not already exists
        #-----------------------------------
        suboutput_dir = OUTPUT_PATH+'//followon_citation_outputs__type_'+ str(type_year)+\
            '__following_'+str(follow_period_length)+'_years__v'+str(VERSION)
        if not os.path.exists(suboutput_dir):
            os.makedirs(suboutput_dir)

        #----------------------------------------------
        # Split the main dataframe based on respective sub-sample and forward to pool
        pool = mp.Pool(n_cores)

        for subframe_identifiers_number in range(len(separated_identifiers_list)):
            #print(subframe_identifiers_number)
            #subframe_identifiers_number = 3

            # collect identifiers for iteration
            subframe_identifiers = separated_identifiers_list[subframe_identifiers_number]

            #***********************************************
            # Save subframe identifiers
            with open(suboutput_dir+'/subframe_identifiers_followon__subframe_' + str(subframe_identifiers_number)+'.pkl', 'wb') as f:
                pickle.dump(subframe_identifiers, f)

            #***********************************************
            # Forward subframe to method
            pool.apply_async(
                        followon_citation_based,
                        args=(
                            follow_period_length,
                            type_year,
                            patent_citation_with_kpss[patent_citation_with_kpss.citation_id.isin(subframe_identifiers)],
                            subframe_identifiers_number,
                            suboutput_dir)
                    )

        pool.close()
        pool.join()

        #--------------------------------------------
        # Collect sub-frames and output under OUTPUT_PATH
        #--------------------------------------------
        print('\t Collect and save subframes for follow-on execution', flush=True)
        file_list = [suboutput_dir+'//'+f for f in os.listdir(suboutput_dir) if \
                 bool(re.search('.csv', f)) & \
                     bool(re.search('Followon_citation_', f)) & \
                         bool(re.search(r'subframe_(\d+)', f))]

        # create dictionary with subframes as keys
        file_dir = {}
        for f in file_list:
            file_dir[int(re.search(r'subframe_(\d+)', f).group(1))] = f

        # Load and concatentate
        master_output_df = pd.DataFrame()
        for k in file_dir.keys():

            append_df = pd.read_csv(file_dir[k], low_memory=False)
            append_df['subframe_number'] = k

            master_output_df = master_output_df.append(
                append_df, ignore_index=True)

        # Save mainframe
        master_output_df.to_csv(OUTPUT_PATH+'//Followon_citation__type_'+str(type_year)+\
                    '__following_'+str(follow_period_length)+'_years__mainDF_v'+str(VERSION)+'.csv',
                    index=False, encoding = 'utf-8')
    else:
        print('\tMain_DF from collecting results from follow-on execution already in output directory; skip', flush=True)

    print('End main routine for follow-on execution', flush=True)
    return

#=======================================================
def main_execution_exploration_exploitation_class_based(num_subframe):
    r'''Splits main data frame 'kpss_ipc_data' and sends to execution function
        num_subframe gives number of unique identifiers in each subframe

        Save in subframes into output subfolder "'exploration_exploitation_classBased__' + \
            'prior_period_reference_'+str(PRIOR_PERIOD_REFERENCE)+'_years__v'+str(VERSION)"
        and concatenated main DF into "'Exploration_exploitation_classBased__'+\
                'prior_period_reference_'+str(PRIOR_PERIOD_REFERENCE)+'_years__mainDF_v'+str(VERSION)"
    r'''

    print('Begin main routine for exploration exploitation-class based execution', flush=True)

    if not('Exploration_exploitation_classBased__prior_period_reference_'+\
           str(PRIOR_PERIOD_REFERENCE)+'_year__mainDF_v'+str(VERSION)+'.csv' in os.listdir(OUTPUT_PATH)):

        # Split the firm identifiers into equally sized portions of num_subframe size
        identifiers_list = kpss_ipc_data.permco.dropna().unique()

        # Sort in ascending order to replicate subframes in predicatable way
        identifiers_list.sort()

        separated_identifiers_list = list(split_list(identifiers_list, num_subframe))

        #-----------------------------------
        # Create suboutput folder if not already exists
        #-----------------------------------
        suboutput_dir = OUTPUT_PATH+'//exploration_exploitation_classBased_outputs__'+ \
            'prior_period_reference_'+str(PRIOR_PERIOD_REFERENCE)+'_years__v'+str(VERSION)
        if not os.path.exists(suboutput_dir):
            os.makedirs(suboutput_dir)

        #----------------------------------------------
        # Split the main dataframe based on respective sub-sample and forward to pool
        pool = mp.Pool(n_cores)

        for subframe_identifiers_number in range(len(separated_identifiers_list)):
            #print(subframe_identifiers_number)
            #subframe_identifiers_number = 3

            # collect identifiers for iteration
            subframe_identifiers = separated_identifiers_list[subframe_identifiers_number]

            #***********************************************
            # Save subframe identifiers
            with open(suboutput_dir+'/subframe_identifiers_class_based__subframe_' + str(subframe_identifiers_number)+'.pkl', 'wb') as f:
                pickle.dump(subframe_identifiers, f)

            #***********************************************
            # Forward subframe to method
            pool.apply_async(
                        exploration_exploitation_class_based,
                        args=(
                            PRIOR_PERIOD_REFERENCE,
                            kpss_ipc_data[kpss_ipc_data.permco.isin(subframe_identifiers)],
                            subframe_identifiers_number,
                            suboutput_dir)
                    )

        pool.close()
        pool.join()

        #--------------------------------------------
        # Collect sub-frames and output under OUTPUT_PATH
        #--------------------------------------------
        print('\t Collect and save subframes for exploration exploitation-class based execution', flush=True)
        file_list = [suboutput_dir+'//'+f for f in os.listdir(suboutput_dir) if \
                     bool(re.search('.csv', f)) & \
                         bool(re.search('Exploration_exploitation_classBased_', f)) & \
                             bool(re.search(r'subframe_(\d+)', f))]

        # create dictionary with subframes as keys
        file_dir = {}
        for f in file_list:
            file_dir[int(re.search(r'subframe_(\d+)', f).group(1))] = f

        # Load and concatentate
        master_output_df = pd.DataFrame()
        for k in file_dir.keys():

            append_df = pd.read_csv(file_dir[k], low_memory=False)
            append_df['subframe_number'] = k

            master_output_df = master_output_df.append(
                append_df, ignore_index=True)

        # Save mainframe
        master_output_df.to_csv(OUTPUT_PATH+'//Exploration_exploitation_classBased__'+\
                    'prior_period_reference_'+str(PRIOR_PERIOD_REFERENCE)+'_year__mainDF_v'+str(VERSION)+'.csv',
                    index=False, encoding = 'utf-8')

    else:
        print('\tMain_DF from collecting results from exploration exploitation-class based execution already in output directory; skip', flush=True)

    print('End main routine for exploration exploitation-class based execution', flush=True)

    return

#============================================================
def main_execution_exploration_exploitation_citation_based(num_subframe):
    r'''Splits main data frame 'patent_citation_with_kpss' and sends to execution function
        num_subframe gives number of unique identifiers in each subframe;

        Save in subframes into output subfolder "'exploration_exploitation_citationBased__' + \
            'prior_period_reference_'+str(PRIOR_PERIOD_REFERENCE)+'_years__v'+str(VERSION)"
        and concatenated main DF into "'Exploration_exploitation_citationBased__'+\
                'prior_period_reference_'+str(PRIOR_PERIOD_REFERENCE)+'_year__mainDF_v'+str(VERSION)"
    r'''

    print('Begin main routine for exploration exploitation-citation based execution', flush=True)

    if not('Exploration_exploitation_citationBased__prior_period_reference_'+\
           str(PRIOR_PERIOD_REFERENCE)+'_year__mainDF_v'+str(VERSION)+'.csv' in os.listdir(OUTPUT_PATH)):

        # Split the firm identifiers into equally sized portions of num_subframe size
        identifiers_list = patent_citation_with_kpss.permco.dropna().unique()

        # Sort in ascending order to replicate subframes in predicatable way
        identifiers_list.sort()

        separated_identifiers_list = list(split_list(identifiers_list, num_subframe))

        #-----------------------------------
        # Create suboutput folder if not already exists
        #-----------------------------------
        suboutput_dir = OUTPUT_PATH+'//exploration_exploitation_citationBased__'+ \
            'prior_period_reference_'+str(PRIOR_PERIOD_REFERENCE)+'_years__v'+str(VERSION)
        if not os.path.exists(suboutput_dir):
            os.makedirs(suboutput_dir)

        #----------------------------------------------
        # Split the main dataframe based on respective sub-sample and forward to pool
        pool = mp.Pool(n_cores)

        for subframe_identifiers_number in range(len(separated_identifiers_list)):
            #print(subframe_identifiers_number)
            #subframe_identifiers_number = 3

            # collect identifiers for iteration
            subframe_identifiers = separated_identifiers_list[subframe_identifiers_number]

            #***********************************************
            # Save subframe identifiers
            with open(suboutput_dir+'/subframe_identifiers_citation_based__subframe_' + str(subframe_identifiers_number)+'.pkl', 'wb') as f:
                pickle.dump(subframe_identifiers, f)

            #***********************************************
            # Forward subframe to method
            pool.apply_async(
                        exploration_exploitation_citation_based,
                        args=(
                            PRIOR_PERIOD_REFERENCE,
                            patent_citation_with_kpss[patent_citation_with_kpss.permco.isin(subframe_identifiers)],
                            subframe_identifiers_number,
                            suboutput_dir)
                    )

        pool.close()
        pool.join()

        #--------------------------------------------
        # Collect sub-frames and output under OUTPUT_PATH
        #--------------------------------------------
        print('\t Collect and save subframes for exploration exploitation-citation based execution', flush=True)
        file_list = [suboutput_dir+'//'+f for f in os.listdir(suboutput_dir) if \
                     bool(re.search('.csv', f)) & \
                         bool(re.search('Exploration_exploitation_citationBased_', f)) & \
                             bool(re.search(r'subframe_(\d+)', f))]

        # create dictionary with subframes as keys
        file_dir = {}
        for f in file_list:
            file_dir[int(re.search(r'subframe_(\d+)', f).group(1))] = f

        # Load and concatentate
        master_output_df = pd.DataFrame()
        for k in file_dir.keys():

            append_df = pd.read_csv(file_dir[k], low_memory=False)
            append_df['subframe_number'] = k

            master_output_df = master_output_df.append(
                append_df, ignore_index=True)

        # Save mainframe
        master_output_df.to_csv(OUTPUT_PATH+'//Exploration_exploitation_citationBased__'+\
                    'prior_period_reference_'+str(PRIOR_PERIOD_REFERENCE)+'_year__mainDF_v'+str(VERSION)+'.csv',
                    index=False, encoding = 'utf-8')

    else:
        print('\tMain_DF from collecting results from exploration exploitation-citation based execution already in output directory; skip', flush=True)

    print('End main routine for exploration exploitation-citation based execution', flush=True)

    return

# %%
#-----------------------------------------------------------
# Parallel Execution
#-----------------------------------------------------------
# Start with frame split of 500 observations

r'''
main_execution_followon(num_subframe=100000,
                        follow_period_length=4,
                        type_year='issue')
main_execution_followon(num_subframe=100000,
                        follow_period_length=5,
                        type_year='issue')
main_execution_followon(num_subframe=100000,
                        follow_period_length=7,
                        type_year='filing')


main_execution_followon_Alice(num_subframe=500000,
                              follow_period_length=5)
r'''

#main_execution_exploration_exploitation_class_based(num_subframe=500)
#main_execution_exploration_exploitation_citation_based(num_subframe=500)



#-----------------------------------------------------------
# Linear Execution
exploration_exploitation_class_based(
    prior_reference_periods=5,
    patent_data_frame=kpss_ipc_data,
    patent_data_frame_counter='complete',
    suboutput_dir=OUTPUT_PATH)
exploration_exploitation_citation_based(
    prior_reference_periods=5,
    patent_data_frame=patent_citation_with_kpss,
    patent_data_frame_counter='complete',
    suboutput_dir=OUTPUT_PATH)

followon_Alice(
    follow_reference_period=5,
    patent_data_frame=patent_citation_with_kpss,
    patent_data_frame_counter='complete',
    suboutput_dir=OUTPUT_PATH)

#r'''
followon_citation_based(
    follow_reference_period=4,
    type_year='issue',
    patent_data_frame=patent_citation_with_kpss,
    patent_data_frame_counter='complete',
    suboutput_dir=OUTPUT_PATH)
followon_citation_based(
    follow_reference_period=5,
    type_year='issue',
    patent_data_frame=patent_citation_with_kpss,
    patent_data_frame_counter='complete',
    suboutput_dir=OUTPUT_PATH)
followon_citation_based(
    follow_reference_period=7,
    type_year='filing',
    patent_data_frame=patent_citation_with_kpss,
    patent_data_frame_counter='complete',
    suboutput_dir=OUTPUT_PATH)
#r'''

r'''
# %%
###############################################
# Local execution
###############################################
with open(r"C:\Users\domin\Desktop\subframe_identifiers_class_based__subframe_6.pkl", 'rb') as f:
    subframe_ids = pickle.load(f)

with open(r"C:\Users\domin\Desktop\subframe_identifiers_citation_based__subframe_0.pkl", 'rb') as f:
    subframe_ids_0 = pickle.load(f)
with open(r"C:\Users\domin\Desktop\subframe_identifiers_citation_based__subframe_1.pkl", 'rb') as f:
    subframe_ids_1 = pickle.load(f)


subframe_df = pd.read_csv(r"C:\Users\domin\Desktop\Exploration_exploitation_classBased_prior5year_1__subframe_6.csv")

citation_based_measure = pd.read_csv(r"C:\Users\domin\Desktop\Exploration_exploitation_citationsBased_prior5year_1__subframe_complete.csv")

set(subframe_ids_0) - set(citation_based_measure.permco.unique())
set(subframe_ids_1) - set(citation_based_measure.permco.unique())
# => all included, but not sure why method starting with '0' frame wouldn't work in parallel execution

len(subframe_df.permco.unique())
len(subframe_ids)

set(subframe_df.permco.unique()) - set(subframe_ids)
set(subframe_ids) - set(subframe_df.permco.unique())
# => perfect fit.
r'''
